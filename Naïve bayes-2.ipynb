{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb897156-76fb-4996-9579-ebe3ab0992ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Probability of Being a Smoker Given Health Insurance Plan\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we use the conditional probability formula.\n",
    "Given:\n",
    "•\tP(Health Insurance)=0.70P(\\text{Health Insurance}) = 0.70P(Health Insurance)=0.70 (Probability of using the health insurance plan)\n",
    "•\tP(Smoker∣Health Insurance)=0.40P(\\text{Smoker} | \\text{Health Insurance}) = 0.40P(Smoker∣Health Insurance)=0.40 (Probability of being a smoker given that the employee uses the health insurance plan)\n",
    "We want to find P(Smoker∣Health Insurance)P(\\text{Smoker} | \\text{Health Insurance})P(Smoker∣Health Insurance), which is already given as 0.40 or 40%. So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.40 or 40%.\n",
    "Q2. Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes\n",
    "•\tBernoulli Naive Bayes:\n",
    "o\tAssumes binary/boolean features.\n",
    "o\tWorks well with features that are represented as either present or absent (e.g., word occurrence in text).\n",
    "o\tEach feature is treated as a binary variable, where the model estimates the probability of each feature being present or absent.\n",
    "•\tMultinomial Naive Bayes:\n",
    "o\tAssumes features are multinomially distributed.\n",
    "o\tWorks well with count data or features that represent the number of occurrences (e.g., term frequency in text).\n",
    "o\tEach feature is treated as a count or frequency, and the model estimates probabilities based on these counts.\n",
    "Q3. How Bernoulli Naive Bayes Handles Missing Values\n",
    "Bernoulli Naive Bayes does not handle missing values directly. Missing values need to be imputed or handled before applying the algorithm. Common strategies for handling missing values include:\n",
    "•\tImputation: Replace missing values with a specific value, such as the mean, median, or a constant.\n",
    "•\tOmission: Exclude instances with missing values from the dataset.\n",
    "Q4. Can Gaussian Naive Bayes Be Used for Multi-Class Classification?\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. It assumes that the features are normally distributed for each class and estimates the probability of each class given the features. The model calculates the likelihood of the features under each class's Gaussian distribution and assigns the class with the highest posterior probability.\n",
    "Q5. Assignment\n",
    "Data Preparation\n",
    "1.\tDownload the Dataset: Download the \"Spambase Data Set\" from UCI Machine Learning Repository.\n",
    "2.\tLoad the Dataset:\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "column_names = [...]  # Add appropriate column names here\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "3.\tPreprocess the Data:\n",
    "o\tHandling Missing Values: Impute or drop missing values if any.\n",
    "o\tFeature Scaling: Standardize or normalize features if required.\n",
    "Implementation\n",
    "1.\tImport Libraries:\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "2.\tSplit the Data:\n",
    "X = data.iloc[:, :-1]  # Features\n",
    "y = data.iloc[:, -1]   # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "3.\tTrain and Evaluate Classifiers:\n",
    "classifiers = {\n",
    "    'BernoulliNB': BernoulliNB(),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'GaussianNB': GaussianNB()\n",
    "}\n",
    "\n",
    "metrics = {'Accuracy': accuracy_score, 'Precision': precision_score, 'Recall': recall_score, 'F1 Score': f1_score}\n",
    "\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    results[name] = {metric_name: metric_func(y_test, y_pred) for metric_name, metric_func in metrics.items()}\n",
    "4.\tReport Performance Metrics:\n",
    "python\n",
    "Copy code\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"  {metric_name}: {score:.4f}\")\n",
    "Discussion\n",
    "•\tPerformance Comparison:\n",
    "o\tCompare the accuracy, precision, recall, and F1 score of each Naive Bayes variant.\n",
    "o\tDetermine which classifier performed best and discuss reasons why.\n",
    "•\tLimitations:\n",
    "o\tDiscuss any limitations of Naive Bayes, such as assumptions about feature independence or performance on certain types of data.\n",
    "Conclusion\n",
    "•\tSummary:\n",
    "o\tSummarize the findings and performance of each classifier.\n",
    "o\tHighlight the best-performing classifier and provide reasons for its performance.\n",
    "•\tSuggestions for Future Work:\n",
    "o\tConsider experimenting with additional preprocessing techniques.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
